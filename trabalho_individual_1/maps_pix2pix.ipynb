{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1365054,"sourceType":"datasetVersion","datasetId":795438}],"dockerImageVersionId":30260,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-02T23:22:34.144841Z","iopub.execute_input":"2022-10-02T23:22:34.145509Z","iopub.status.idle":"2022-10-02T23:22:34.197734Z","shell.execute_reply.started":"2022-10-02T23:22:34.145373Z","shell.execute_reply":"2022-10-02T23:22:34.196664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\nclass MapsTransform:\n    def __init__(self, mirror_prob: float = 0):\n        self.mirror_prob = mirror_prob\n        \n    def __jitter(self, x, y):\n        torch.manual_seed(42)\n        random_crop = T.RandomCrop((256, 256))\n        x, y = random_crop(x), random_crop(y)\n        torch.manual_seed(0)\n        return x, y\n        \n                \n    def __call__(self, x, y):\n        \n        # resizing\n        x, y = TF.resize(x, [286, 286]), TF.resize(y, [286, 286])\n        \n        # random jittering\n        x, y = self.__jitter(x, y)\n        \n        # horizontal flip\n        r = np.random.uniform()\n        if r <= self.mirror_prob:\n            x, y = TF.hflip(x), TF.hflip(y)\n        \n        return {\n            \"areal\": x,\n            \"map_pic\": y\n        }","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:22:34.199814Z","iopub.execute_input":"2022-10-02T23:22:34.200434Z","iopub.status.idle":"2022-10-02T23:22:36.479915Z","shell.execute_reply.started":"2022-10-02T23:22:34.200397Z","shell.execute_reply":"2022-10-02T23:22:36.478949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\n\nclass MapsDataset(Dataset):\n    def __init__(self, img_folder: str, transform = None):\n        self.img_folder = img_folder\n        self.imgs = os.listdir(img_folder)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, idx):\n        img_name = self.imgs[idx]\n        img = os.path.join(self.img_folder, img_name)\n        img = Image.open(img)\n        img = TF.to_tensor(img)\n        width = img.shape[2] // 2\n        areal, map_pic = img[:, :, :width], img[:, :, width:]\n        if self.transform != None:\n            transformed = self.transform(areal, map_pic)\n            areal, map_pic = transformed[\"areal\"], transformed[\"map_pic\"]\n        areal = TF.normalize(areal, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        map_pic = TF.normalize(map_pic, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        return {\n            \"areal\": areal,\n            \"map_pic\": map_pic\n        }","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:03.939359Z","iopub.execute_input":"2022-10-02T23:44:03.939753Z","iopub.status.idle":"2022-10-02T23:44:03.951414Z","shell.execute_reply.started":"2022-10-02T23:44:03.939716Z","shell.execute_reply":"2022-10-02T23:44:03.950369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=1):\n        super().__init__()\n        self.batch_size=batch_size\n        \n    def setup(self, stage=None):\n        transform = MapsTransform(0.5)\n        self.train_dataset = MapsDataset(\"../input/pix2pix-maps/train\", transform)\n        self.validation_dataset = MapsDataset(\"../input/pix2pix-maps/val\", transform)\n        \n    def train_dataloader(self):\n        train_loader = DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True\n        )\n        return train_loader\n    \n    def val_dataloader(self):\n        val_loader = DataLoader(\n            self.validation_dataset,\n            batch_size=self.batch_size,\n            shuffle=False\n        )\n        return val_loader","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:04.148437Z","iopub.execute_input":"2022-10-02T23:44:04.148758Z","iopub.status.idle":"2022-10-02T23:44:04.155913Z","shell.execute_reply.started":"2022-10-02T23:44:04.14873Z","shell.execute_reply":"2022-10-02T23:44:04.154681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = MapsDataset(\"../input/pix2pix-maps/train\", MapsTransform(1))\na = ds[101]\nareal, map_pic = a[\"areal\"], a[\"map_pic\"]\nareal = areal.numpy()\nareal = np.moveaxis(areal, 0, 2)\nimport matplotlib.pyplot as plt\n\nplt.imshow(areal)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:04.325538Z","iopub.execute_input":"2022-10-02T23:44:04.326192Z","iopub.status.idle":"2022-10-02T23:44:04.575983Z","shell.execute_reply.started":"2022-10-02T23:44:04.326153Z","shell.execute_reply":"2022-10-02T23:44:04.575062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"map_pic = map_pic.numpy()\nmap_pic = np.moveaxis(map_pic, 0, 2)\nplt.imshow(map_pic)","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:06.955642Z","iopub.execute_input":"2022-10-02T23:44:06.956013Z","iopub.status.idle":"2022-10-02T23:44:07.19077Z","shell.execute_reply.started":"2022-10-02T23:44:06.95598Z","shell.execute_reply":"2022-10-02T23:44:07.189954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.min(map_pic)","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:09.65979Z","iopub.execute_input":"2022-10-02T23:44:09.660328Z","iopub.status.idle":"2022-10-02T23:44:09.672798Z","shell.execute_reply.started":"2022-10-02T23:44:09.660294Z","shell.execute_reply":"2022-10-02T23:44:09.672083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DownBlock(nn.Module):\n    def __init__(\n        self,\n        num_channels: int,\n        num_filters: int,\n        use_norm: bool = True,\n        last_layer: bool = False\n    ):\n        super().__init__()\n        stride = 1 if last_layer else 2\n        self.conv = nn.Conv2d(num_channels, num_filters, 4, stride, 1)\n        self.norm = nn.InstanceNorm2d(num_filters, affine=True) if use_norm else nn.Identity()\n        self.leaky = nn.LeakyReLU(0.2)\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        return self.leaky(x)\n    \nclass UpBlock(nn.Module):\n    def __init__(\n        self,\n        num_channels: int,\n        num_filters: int,\n        use_dropout: bool = False\n    ):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(num_channels, num_filters, 4, 2, 1)\n        self.relu = nn.ReLU()\n        self.norm = nn.InstanceNorm2d(num_filters, affine=True)\n        self.drop = nn.Dropout(0.5) if use_dropout else nn.Identity()\n    \n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.drop(x)\n        return self.relu(x)\n        \n        \n","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:09.856971Z","iopub.execute_input":"2022-10-02T23:44:09.857559Z","iopub.status.idle":"2022-10-02T23:44:09.873826Z","shell.execute_reply.started":"2022-10-02T23:44:09.857517Z","shell.execute_reply":"2022-10-02T23:44:09.872841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, input_channels: int = 3):\n        super().__init__()\n        self.down1 = DownBlock(input_channels * 2, 64, use_norm=False)\n        self.down2 = DownBlock(64, 128)\n        self.down3 = DownBlock(128, 256)\n        self.down4 = DownBlock(256, 512, last_layer=True)\n        self.final = nn.Conv2d(512, 1, kernel_size=4, padding=1)\n    \n    def forward(self, x, y):\n        x = torch.cat([x, y], dim=1)\n        x = self.down1(x)\n        x = self.down2(x)\n        x = self.down3(x)\n        x = self.down4(x)\n        \n        return self.final(x)\n                \n\nclass Generator(nn.Module):\n    def __init__(self, input_channels: int = 3):\n        super().__init__()\n        self.input_channels = input_channels\n        self.down1 = DownBlock(input_channels, 64, use_norm=False)\n        self.down2 = DownBlock(64, 128)\n        self.down3 = DownBlock(128, 256)\n        self.down4 = DownBlock(256, 512)\n        self.down5 = DownBlock(512, 512)\n        self.down6 = DownBlock(512, 512)\n        self.down7 = DownBlock(512, 512)\n        \n        self.bottleneck = DownBlock(512, 512, last_layer=True, use_norm=False)\n        \n        self.up1 = UpBlock(512, 512, True)\n        self.up2 = UpBlock(1024, 512, True)\n        self.up3 = UpBlock(1024, 512, True)\n        self.up4 = UpBlock(1024, 512)\n        self.up5 = UpBlock(1024, 256)\n        self.up6 = UpBlock(512, 128)\n        self.up7 = UpBlock(256, 64)\n        self.up8 = nn.ConvTranspose2d(128, input_channels, 4, 2, 1)\n        self.tanh = nn.Tanh()\n        \n    def forward(self, x):\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        \n        btn = self.bottleneck(d7)\n        \n        up1 = self.up1(btn)\n        up2 = self.up2(torch.concat([up1, d7], dim=1))\n        up3 = self.up3(torch.concat([up2, d6], dim=1))\n        up4 = self.up4(torch.concat([up3, d5], dim=1))\n        up5 = self.up5(torch.concat([up4, d4], dim=1))\n        up6 = self.up6(torch.concat([up5, d3], dim=1))\n        up7 = self.up7(torch.concat([up6, d2], dim=1))\n        up8 = self.up8(torch.concat([up7, d1], dim=1))\n        return self.tanh(up8)","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:10.029879Z","iopub.execute_input":"2022-10-02T23:44:10.03021Z","iopub.status.idle":"2022-10-02T23:44:10.047768Z","shell.execute_reply.started":"2022-10-02T23:44:10.03018Z","shell.execute_reply":"2022-10-02T23:44:10.046806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_weights(m):\n    if isinstance(m, DownBlock):\n        nn.init.normal_(m.conv.weight, 0.0, 0.02)\n        if isinstance(m.norm, nn.InstanceNorm2d):\n            nn.init.normal_(m.norm.weight, 0.0, 0.02)\n            \n    elif isinstance(m, UpBlock):\n        nn.init.normal_(m.conv.weight, 0.0, 0.02)\n        nn.init.normal_(m.norm.weight, 0.0, 0.02)\n        \n    elif isinstance(m, nn.Conv2d):\n        nn.init.normal_(m.weight, 0.0, 0.02)\n        \n        \ndef display_results(x, y, y_hat, current_epoch: int, path: str,figsize=(10,5)):\n    x = x.detach().cpu().permute(1, 2, 0)\n    y = y.detach().cpu().permute(1, 2, 0)\n    y_hat = y_hat.detach().cpu().permute(1, 2, 0)\n    \n    x, y, y_hat = x.float(), y.float(), y_hat.float()\n    \n    images = [x, y, y_hat]\n    titles = ['input','real','generated']\n    print(f'Epoch: {current_epoch}')\n    fig, ax = plt.subplots(1, 3, figsize=figsize)\n    for idx,img in enumerate(images):\n        ax[idx].imshow(img)\n        ax[idx].axis(\"off\")\n    for idx, title in enumerate(titles):    \n        ax[idx].set_title('{}'.format(title))\n    plt.savefig(path)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:10.179892Z","iopub.execute_input":"2022-10-02T23:44:10.180193Z","iopub.status.idle":"2022-10-02T23:44:10.191527Z","shell.execute_reply.started":"2022-10-02T23:44:10.180166Z","shell.execute_reply":"2022-10-02T23:44:10.190621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = Generator()\nm.apply(init_weights)\na = m(torch.rand(1, 3, 256, 256))\na.shape","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:10.352878Z","iopub.execute_input":"2022-10-02T23:44:10.353192Z","iopub.status.idle":"2022-10-02T23:44:11.583315Z","shell.execute_reply.started":"2022-10-02T23:44:10.353164Z","shell.execute_reply":"2022-10-02T23:44:11.582195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = Discriminator()\nm.apply(init_weights)\na = m(torch.rand(1, 3, 256, 256), torch.rand(1, 3, 256, 256))\na.shape","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:11.585409Z","iopub.execute_input":"2022-10-02T23:44:11.585872Z","iopub.status.idle":"2022-10-02T23:44:11.73841Z","shell.execute_reply.started":"2022-10-02T23:44:11.585834Z","shell.execute_reply":"2022-10-02T23:44:11.737443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Pix2Pix(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.save_hyperparameters()\n        self.discriminator = Discriminator(3)\n        self.generator = Generator(3)\n        self.generator.apply(init_weights)\n        self.discriminator.apply(init_weights)\n        self.bce = nn.BCEWithLogitsLoss()\n        self.l1 = nn.L1Loss()\n\n    def forward(self, x):\n        return self.generator(x)\n    \n    def training_step(self, batch, batch_idx, optimizer_idx):\n        x, y = batch[\"areal\"], batch[\"map_pic\"]\n        \n        if optimizer_idx == 0:\n            y_fake = self(x)\n            D_real = self.discriminator(x, y)\n            D_fake = self.discriminator(x, y_fake.detach())\n            D_real_loss = self.bce(D_real, torch.ones_like(D_real))\n            D_fake_loss = self.bce(D_fake, torch.zeros_like(D_fake))\n            loss = (D_real_loss + D_fake_loss) / 2\n            self.log('Discriminator loss', loss)\n        \n        elif optimizer_idx == 1:\n            y_fake = self(x)\n            D_fake = self.discriminator(x, y_fake)\n            loss = self.bce(D_fake, torch.ones_like(D_fake))\n            l1_loss = self.l1(y_fake, y) * 200\n            loss += l1_loss\n            self.log('Generator loss', loss)\n            \n        if self.current_epoch % 20 == 0 and batch_idx == 0 and optimizer_idx == 1:\n            y_hat = self(x).detach()\n            display_results(\n                x[0],\n                y[0],\n                y_hat[0],\n                self.current_epoch,\n                \"/kaggle/working/img_{}\".format(self.current_epoch)\n            )\n        \n        return loss\n\n    \n    def configure_optimizers(self):\n        generator_opt = torch.optim.Adam(self.generator.parameters(),  0.0002, betas=(0.5, 0.999))\n        discriminator_opt = torch.optim.Adam(self.discriminator.parameters(),  0.0002, betas=(0.5, 0.999))\n        return [discriminator_opt, generator_opt]\n        ","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:11.73971Z","iopub.execute_input":"2022-10-02T23:44:11.740012Z","iopub.status.idle":"2022-10-02T23:44:11.753208Z","shell.execute_reply.started":"2022-10-02T23:44:11.739985Z","shell.execute_reply":"2022-10-02T23:44:11.7521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = MapsTransform(0.5)\ndataset = MapsDataset(\"../input/pix2pix-maps/train\", transform)\ntrain_dataloader = DataLoader(\n    dataset,\n    batch_size=1,\n    shuffle=True,\n    num_workers=2\n)\nmodel = Pix2Pix()\n\ntrainer = pl.Trainer(\n    accelerator=\"auto\",\n    precision=16,\n    devices=1 if torch.cuda.is_available() else None,  # limiting got iPython runs\n    max_epochs=200,\n    callbacks=[pl.callbacks.TQDMProgressBar(refresh_rate=20)],\n    default_root_dir=\"./\",\n)\n\ntrainer.fit(\n    model=model,\n    train_dataloaders=train_dataloader\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-02T23:44:11.755567Z","iopub.execute_input":"2022-10-02T23:44:11.756055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}